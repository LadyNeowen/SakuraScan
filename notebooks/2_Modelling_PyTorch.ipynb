{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ac10b5-86f4-457a-be04-ba0551dfc884",
   "metadata": {},
   "source": [
    "# SakuraScan – Modelling (PyTorch)\n",
    "\n",
    "## Objectives\n",
    "- Train a binary image classifier to distinguish healthy cherry leaves from leaves with powdery mildew.\n",
    "- Use transfer learning with a pretrained CNN (ResNet18) for robust performance.\n",
    "- Save the trained model for use in the SakuraScan Streamlit dashboard.\n",
    "\n",
    "## Inputs\n",
    "- Image dataset stored in `Data/source_images/healthy` and `Data/source_images/powdery_mildew`.\n",
    "\n",
    "## Outputs\n",
    "- Trained PyTorch model weights saved to `app_pages/src/models/sakuramodel_resnet18.pth`.\n",
    "- Printed training and validation accuracy and loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8deb862a-98ba-422d-be27-665e19ca2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model training script for SakuraScan using PyTorch and transfer learning.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dda1ab5a-fe07-4097-89aa-e0f40fe844d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Set up paths, constants, and devive configuration.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Project root is the parent folder of the notebooks directory\n",
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"Data\" / \"source_images\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"app_pages\" / \"src\" / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODEL_DIR / \"sakuramodel_resnet18.pth\"\n",
    "\n",
    "BATCH_SIZE = 32  # Number of images processed in one training step.\n",
    "NUM_EPOCHS = 8  # How many full passes the model makes over the entire training dataset.\n",
    "LEARNING_RATE = 1e-4  # Controls how big the weight updates are during training.\n",
    "VAL_SPLIT = 0.2  # Fraction of the dataset reserved for validation to evaluate model performance.\n",
    "IMAGE_SIZE = 224  # Target resolution for all input images (ResNet models expect 224×224 pixels).\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db46169-475f-46c8-86c6-26466dc1060b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['healthy', 'powdery_mildew']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create training and validation datasets and dataloaders using ImageFolder.\n",
    "\"\"\"\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Only resize + normalize for validation\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Load all images with a temporary transform (updated per split)\n",
    "full_dataset = datasets.ImageFolder(root=str(DATA_DIR), transform=train_transform)\n",
    "\n",
    "class_names: List[str] = full_dataset.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10493c55-38d0-4d47-868e-dfb3720ad128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions for training and evaluating the model.\n",
    "\"\"\"\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch and return loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation data and return loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / total, correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40a4ff08-ce5c-40d2-89a2-b8adce963f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.3%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\NeosT/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a ResNet18 model for binary classification.\n",
    "\"\"\"\n",
    "\n",
    "def create_model(num_classes):\n",
    "    \"\"\"\n",
    "    Load a pretrained ResNet18 and replace the final layer.\n",
    "    \"\"\"\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "    # Freeze backbone to focus training on final layer\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the classifier\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model(len(class_names))\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac53ac6c-0cbf-48a4-a648-59e44536edc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3367, 841)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create training and validation dataloaders from the full dataset.\n",
    "\"\"\"\n",
    "\n",
    "dataset_size = len(full_dataset)\n",
    "val_size = int(dataset_size * VAL_SPLIT)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "len(train_loader.dataset), len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e5e2c8f-9ee7-416e-89f4-fd29c7ae4aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8 | Train Loss: 0.4909 Acc: 0.8402 | Val Loss: 0.3452 Acc: 0.9631\n",
      "Epoch 2/8 | Train Loss: 0.2773 Acc: 0.9849 | Val Loss: 0.2116 Acc: 0.9941\n",
      "Epoch 3/8 | Train Loss: 0.1980 Acc: 0.9878 | Val Loss: 0.1440 Acc: 0.9941\n",
      "Epoch 4/8 | Train Loss: 0.1455 Acc: 0.9941 | Val Loss: 0.1131 Acc: 0.9964\n",
      "Epoch 5/8 | Train Loss: 0.1201 Acc: 0.9938 | Val Loss: 0.0874 Acc: 1.0000\n",
      "Epoch 6/8 | Train Loss: 0.1019 Acc: 0.9926 | Val Loss: 0.0760 Acc: 0.9964\n",
      "Epoch 7/8 | Train Loss: 0.0821 Acc: 0.9964 | Val Loss: 0.0595 Acc: 1.0000\n",
      "Epoch 8/8 | Train Loss: 0.0704 Acc: 0.9964 | Val Loss: 0.0529 Acc: 0.9988\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the model for several epochs and print loss and accuracy.\n",
    "\"\"\"\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    val_loss, val_acc = evaluate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"train_acc\"].append(train_acc)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "    history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} \"\n",
    "        f\"| Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} \"\n",
    "        f\"| Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9d352e1-00c2-4c3a-ab82-22d772246759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: C:\\Users\\NeosT\\OneDrive\\Skrivbord\\VsCode-Projects\\SakuraScan\\SakuraScan\\app_pages\\src\\models\\sakuramodel_resnet18.pth\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save the trained model weights and metadata for later use.\n",
    "\"\"\"\n",
    "\n",
    "def save_model(model, path, class_names, image_size):\n",
    "    \"\"\"\n",
    "    Save the model state_dict and basic metadata to a .pth file.\n",
    "    \"\"\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"class_names\": class_names,\n",
    "            \"image_size\": image_size,\n",
    "            \"arch\": \"resnet18\",\n",
    "        },\n",
    "        path,\n",
    "    )\n",
    "    print(f\"Model saved to: {path}\")\n",
    "\n",
    "\n",
    "save_model(model, MODEL_PATH, class_names, IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804dea0-8482-45ac-bad4-76f37f19466c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
